---
title: "Day 3"
author: "Taylor R. Brown"
date: "11/4/2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Outline

- 2.1 general definitions for Markov chains 
  * we won't use all of them
  
- 2.2 definitions for HMMs/SSMs 
  * most of these are worth knowing

## Key definitions

These aren't critical for the course, because we won't be operating at the measure-theoretic level. However, they are worth knowing.

- probability space
- measurable space
- stochastic process
- filtration
- filtered space, filtered probability space
- sigma field generated by a filtration (unions of sigma fields are not sigma fields)
- stochastic process adapted to a filtration, "natural" filtrations

## Key definitions

These things aren't absolutely critical for this course, either.

- shift operator (only makes sense on infinite-dimensional space)
- for coordinate processes $X_k \circ \theta^n = X_{k+n}$ (visualize the whole sequence) 
- Markov properties (weak and strong)
- reverse kernel, chains that are reversible, stationarity (important for MCMC, but not as much for us)


## Key definitions help

These are more important. Most of you understand these concepts already--only the notation is new.

- kernels 
  * unnormalized transition versus Markov
  * more familiar notation (e.g. conditional densities, conditional pmfs)
  * product kernels
  
- positive measures operating on kernels
  * marginal measures
  * product measures

- kernels operating on functions
  * one-step-ahead conditional expectations
  * a law of total expectation
  

## Definition of HMM

A HMM is a two-piece Markov chain, $\{X_k, Y_k\}_{k \ge 0}$, where the first series is unobservable and the second is observable, that has the transition kernel
$$
T[(x,y), C] = \iint_C Q(x,dx')G(x',dy') \\
\hspace{10mm}(x,y) \in \mathsf{X} \times \mathsf{Y}, C \in \mathcal{X} \otimes \mathcal{Y}
$$
and initial distribution $\nu \otimes G$.

- $Q$ is a Markov kernel from $(\mathsf{X},\mathcal{X})$ to $(\mathsf{X},\mathcal{X})$
- $G$ is a Markov kernel from $(\mathsf{X},\mathcal{X})$ to $(\mathsf{Y},\mathcal{Y})$

## Definition of HMM

A HMM is **partially dominated** of $G(x,\cdot)$ is dominated by some $\mu(\cdot)$ for each $x \in \mathsf{X}$.

In alternative lingo: $G$ has a density or a pmf. E.g.

- $G(x, A) = \int_A g(y|x)dy$

- $G(x, A) = \sum_{y \in A} g(y|x)$


## Definition of HMM

A HMM is **fully dominated** if it is partially dominated, and if $Q(x,\cdot)$ is dominated by some $\lambda(\cdot)$ for each $x \in \mathsf{X}$, and if that $\lambda$ also dominates $\nu$.

In alternative lingo: $Q$ has a transition density or a transition pmf. E.g.


- $Q(x, A) = \int_A q(x'|x)dx'$

- $Q(x, A) = \sum_{x' \in A} q(x'|x)$

We exclusively deal with HMMs that are fully dominated in this class, but we continue to write integrals like $\int Q(x,dx')$ instead of $\int q(x' \mid x) dx'$ or $\sum_{x' \in A} q(x'|x)$ because a state vector might involve both discrete and continuous components.

## Proving Conditional Independence of the Observations

Hints for page 44 proof:

- The less formal way to prove this is to just write the integral and see how the integrand separates out. The formal way requires the measure-theoretic definition of a conditional expectation.

- By one definition, $E_{\nu}\left[ \prod_{i=1}^p f_i(Y_{k_i}) \bigg\rvert X_{k_1}, \ldots, X_{k_p} \right]$ is any function satisfying 
$$
E_{\nu}\left[h(X_{k_1}, \ldots, X_{k_p}) E_{\nu}\left[ \prod_{i=1}^p f_i(Y_{k_i}) \bigg\rvert X_{k_1}, \ldots, X_{k_p} \right] \right] = E_{\nu}\left[h(X_{k_1}, \ldots, X_{k_p}) \prod_{i=1}^p f_i(Y_{k_i}) \right]
$$
for any measurable $h$. You just need to show that $\prod_{i=1}^p \int_{\mathsf{Y}} f_i(y) G(X_{k_{i}},dy)$ is one such function!

## Proving Conditional Independence of the Observations

More hints for page 44 proof:

- tell the difference between $i=1,\ldots,k_p$ and $k_1, \ldots, k_p$. The former is a superset.

- The only simplification that isn't a rearrangement in the proof is that in the last step they integrate out the $y_{i}$s where $i \notin \{k_1, \ldots, k_p\}$.


## Conditional Independence Summary

Let $k \neq k_1, \ldots, k_p$...

- $E_{\nu}\left[\prod_i f_i(y_{k_i}) \mid X_{k_1}, \ldots, X_{k_p} \right] = \prod_iE_{\nu}\left[ f_i(y_{k_i}) \mid X_{k_1}, \ldots, X_{k_p} \right]$

- $E_{\nu}\left[f(y_k) h(X_{k_1}, \ldots, X_{k_p}) \mid X_k \right] = E_{\nu}\left[f(y_k) \mid X_k \right]E_{\nu}\left[ h(X_{k_1}, \ldots, X_{k_p}) \mid X_k \right]$ 

- $E_{\nu}\left[ f(Y_{k_1}, \ldots, Y_{k_p}) \mid X_{k_1}, \ldots, X_{k_p}, X_{k} \right] = E_{\nu}\left[ f(Y_{k_1}, \ldots, Y_{k_p}) \mid X_{k_1}, \ldots, X_{k_p} \right]$
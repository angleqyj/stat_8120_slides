---
title: "Day 2"
author: "Taylor R. Brown"
date: "10/3/2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Examples

The first question people ask about a state space model, or SSMs in general:

What does the state variable represent?

It always depends. The point of this section is to describe enough examples to demonstrate how limitless the potential is.


## Examples

We show examples that are 

- finite state spaces
- continuous state spaces
- models that don't require particle filtering
- models that do require particle filtering

## Example 1.3.1: Gilbert-Elliott Channel Model

- $k$ is the time index
- $S_k \in \{0,1\}$ hidden Markov chain, represents whether low/high error conditions
- $B_k \in \{0,1\}$ hidden iid $\text{Bernoulli}(.5)$ rvs, represent true bits in a message
- $X_k = (B_k,S_k)$ hidden 2-d state vector at time $k$
- $Y_k \in \{0,1\}$ observed bit in a message at time $k$
- allows for "bursty" errors
- $B_k$ and $S_k$ are independent. $S_k$ only affects observation errors, i.e.

$$
P(Y_k = b \mid B_k = b, S_k = s) = 1-q_s
$$

## Example 1.3.4 (Capture-Recapture)

- only looking at animals that have been tagged **at least once**
- $X_k \in \{1,2,3,\dagger\}$ hidden Markov chain represents geographical zone of a lizard ($\dagger$ means dead)
- $Y_k \in \{0,1,2,3\}$ represents which observed geographical zone ($0$ represents unobserved)
- $P(Y_k = 0 \mid X_k = \dagger) = 1$

Parameter inference gives us transportation schedules of animals, as well as probability of dying in certain areas. It can also be used to give us estimate of overall population size. 

## Example 1.3.6 (Speech Recognition)

Example(?): Facebook's phone app monitors your speech in realtime and tries to recognize when you say a client company's product's name

- $X_k \in \{1,\ldots,k\}$ which phoneme someone is actually saying
- $Y_k \sim \sum_{i} \alpha_i \text{MVN}(\mu_i, \Sigma_i)$ [observed summaries of a Fourier transform of a window of the speech signal](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)
- Usually each word gets its own model, and you evaluate each model's likelihood on a given speech recording

## Example 1.3.8 (Noisy or Noiseless) Autoregressive Models

Write 
$$
Z_{k} = \phi_1 Z_{k-1} + \cdots + \phi_p Z_{k-p} + U_k
$$
as 
$$
X_k = 
\begin{bmatrix}
\phi_1 & \phi_2 & \cdots & \phi_{p-1} & \phi_p \\
1 & 0 & \cdots & 0 & 0\\
0 & 1 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & \cdots & 0&1 & 0
\end{bmatrix}
X_{k-1} + 
\begin{bmatrix}
U_k \\
0 \\
0 \\
\vdots \\
0
\end{bmatrix}
$$
where $X_k = (Z_k, \ldots, Z_{k-p+1})^\intercal$

and either $Y_k = \begin{bmatrix}1 & 0 & \cdots & 0 \end{bmatrix}X_k$ or $Y_k = \begin{bmatrix}1 & 0 & \cdots & 0 \end{bmatrix}X_k + V_k$


## Example 1.3.10 (Change Point Detection) 

- $Y_k$ the observed response

- $W_k$ the unobserved "true" response

- $C_k \in \{0,1\}$ unobserved, whether or not there was a change point at $k$

- $V_k$, $U_k$ iid Gaussian noise

- see page 21 for plot

observation equation:
$$
Y_k = W_k + V_k
$$

$$
W_{k+1} = A(C_{k+1})W_k + R(C_{k+1})U_{k}
$$

where 

- $A(0) = I$, and $A(1) = 0$
- $R(1) = R$ and $R(0) = 0$


## Example 1.3.12 (Bearings-only Tracking)

- we only observe the angle of the direction of the ship/airplane/etc. ($Y_k$)
- we would like to have an estimate of its unobserved location $(P_{x,k}, P_{y,k})$ at time $kT$

The hidden state has the following dynamics
$$
X_k = 
\begin{bmatrix}
P_{x,k} \\
\dot{P}_{x,k} \\
P_{y,k} \\
\dot{P_{y,k}}
\end{bmatrix}
=
\begin{bmatrix}
1 & T & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & T \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
P_{x,k-1} \\
\dot{P}_{x,k-1} \\
P_{y,k-1} \\
\dot{P}_{y,k-1}
\end{bmatrix}
+
\sigma_U
\begin{bmatrix}
T^2/2 & 0 \\
T & 0 \\
0 & T^2/2 \\
0 & T
\end{bmatrix}
\begin{bmatrix}
U_{k,1}\\
U_{k,2}
\end{bmatrix}
$$

Assuming we are standing at coordinate $(0,0)$, the observation equation is
$$
Y_k = \tan^{-1} \left(\frac{P_{y,k}-0 }{P_{y,k} - 0} \right) + \sigma_V V_k
$$

## Example 1.3.12 (Bearings-only Tracking)

A cool particle filtering animation I saw on Twitter:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Particle filters are general algorithms for inferring the state of a system with noisy dynamics and noisy measurements. Here&#39;s an example with a robot in a circular room. Red=true robot, blue=guesses, occasional red line=noisy range sensor measurement. Details in thread 1/ <a href="https://t.co/1UnJjnJYPT">pic.twitter.com/1UnJjnJYPT</a></p>&mdash; Andrew M. Webb (@AndrewM_Webb) <a href="https://twitter.com/AndrewM_Webb/status/1184559073913704448?ref_src=twsrc%5Etfw">October 16, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 


## Example 1.3.12 ( Bearings-only Tracking)

Tracking (with more than just the bearings measurements) is a common problem in computer-vision and robotics.

Usually these models assume simple state dynamics, but complicated observation densities (allow for occlusion, shape changes, color changes, etc.)

[video](https://www.youtube.com/embed/qTD8yVJyhfs)

## Example 1.3.13 (Stochastic Volatility)

- $S_t$ the price of the S\&P500 at time $t$
- $Y_t = \log(S_t) - \log(S_{t-1})$ is the log return from $t-1$ to $t$

## Example 1.3.13 (Stochastic Volatility)

```{r, echo=T, results = "hide"}
library(quantmod)
getSymbols("SPY")
st <- Ad(SPY)
logReturns <- diff(log(st))
par(mfrow=c(2,1))
plot.ts(st)
plot.ts(logReturns)
par(mfrow=c(1,1))
```


## Example 1.3.13 (Stochastic Volatility)

The "volatility" appears to "cluster". The state here is the logarithm of the volatility, and follows an AR(1) process:

$$
X_{k+1} = \phi X_k + \sigma U_k
$$

The observation equation is

$$
Y_k = \underbrace{\beta \exp\left[X_k/2 \right]}_{\text{changing scale/width}} V_k
$$

where $V_k$ is iid Gaussian noise
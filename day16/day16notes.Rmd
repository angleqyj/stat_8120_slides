---
title: "Day 16"
author: "Taylor R. Brown"
date: "1/3/2020"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Roadmap

9.1

- back to non-time series situation

- importance sampling -> **self-normalized importance sampling**

- mostly proof-free (classical asymptotics)

9.2

- sampling importance resampling

9.3

- sequential importance sampling with resampling (single-step analysis)

9.4

- sequential importance sampling with resampling (multi-step analysis)


## What's different about **self-normalized** estimators

In the last example, we successfully targeted a normal distribution with a Cauchy distribution.

This was a useful **variance-reduction** technique, however, we will often be interested in the case where our target distribution is complicated, where we won't be able to evaluate the density.

When this happens, it is still often possible to evaluate an unnormalized version of the target (e.g. the posterior in Bayesian statistics, or the filtering distributions in particle filtering.)

##  **Self-Normalized** Importance Sampling

Just as before, we sample $\xi^1, \xi^2, \ldots, \overset{\text{iid}}{\sim} \nu$, and then use them to evaluate

$$
\hat{\mu}_{\nu,N}^{\text{IS}}\left( f \right) = \frac{ \sum_{i=1}^N \frac{d\mu}{d\nu}(\xi^i)  f(\xi^i)}{ \sum_{j=1}^N \frac{d\mu}{d\nu}(\xi^j) }
$$



##  **Self-Normalized** Importance Sampling

$\nu \left[\frac{d\mu}{d\nu} \right] = 1$ still, but we cannot evaluate $\frac{d\mu}{d\nu}(x)$. If we can evaluate some function $g(x)$ where
$$
\frac{d\mu}{d\nu}(x) = \text{constant} \times g(x)
$$
then we **can** evaluate the self-normalized weights because the normalizing constant will cancel out of

$$
\frac{ \frac{d\mu}{d\nu}(\xi^i) }{ \sum_{j=1}^N \frac{d\mu}{d\nu}(\xi^j) } = \frac{\text{constant} \times g(\xi^i) }{ \text{constant} \times \sum_{j=1}^N g(\xi^j) } 
$$

However, we will not write $g$ when working on the math, so keep in mind that $\nu \left[\frac{d\mu}{d\nu} \right] = 1$, and $\frac{d\mu}{d\nu}(x)$ is not the tractable function we are evaluating on each sample.


## Theorem 9.1.8 (part 1)

Let $f$ be a measurable function such that $\mu(|f|) < \infty$. Assume that $\mu \ll \nu$, and let $\xi^1, \xi^2, \ldots, \overset{\text{iid}}{\sim} \nu$. Then

$$
\hat{\mu}_{\nu,N}^{\text{IS}}\left( f \right) \overset{\nu\text{-as}}{\to} \mu(f)
$$
as $N \to \infty$.


## Theorem 9.1.8 (part 2)

If in addition

1. 
$$
\nu\left[ \left( \frac{d\mu}{d\nu}(\xi^i) \right)^2 \right] < \infty
$$
and

2. 
$$
\nu\left[ \left( f(\xi^i) \frac{d\mu}{d\nu}(\xi^i) \right)^2 \right] < \infty
$$
Then


$$
\sqrt{N}\left[ \hat{\mu}_{\nu,N}^{\text{IS}}\left( f \right)  - \mu(f) \right] \overset{\text{D}}{\to} \text{Normal}\left[0, \sigma^2\left( \nu, f \right) \right]
$$
where 
$$
\sigma^2\left( \nu, f \right) = \text{Var}_{\nu}\left[  \frac{d\mu}{d\nu}(\xi)[f(\xi)-\mu(f)]  \right] = \text{E}_{\nu}\left[  \left[\frac{d\mu}{d\nu}\right]^2 \left[f(\xi)-\mu(f) \right]^2  \right]
$$
is the asymptotic variance for the expectation $\mu(f)$ while using $\nu$.


## Theorem 9.1.8: proof (part 1)

$$
\hat{\mu}_{\nu,N}^{\text{IS}}\left( f \right) = \frac{ \sum_{i=1}^N \frac{d\mu}{d\nu}(\xi^i)  f(\xi^i)}{ \sum_{j=1}^N \frac{d\mu}{d\nu}(\xi^j) } = \frac{ N^{-1} \sum_{i=1}^N \frac{d\mu}{d\nu}(\xi^i)  f(\xi^i)}{N^{-1} \sum_{j=1}^N \frac{d\mu}{d\nu}(\xi^j) }
$$

Numerator converges to $\mu(f)$ by SLLN. Denominator converges to $\mu(\mathsf{X}) = 1$ by SLLN. So the ratio converges to $\mu(f)$. 


## Theorem 9.1.8: proof (part 2)

\begin{align*}
\sqrt{N}\left[ \hat{\mu}_{\nu,N}^{\text{IS}}\left( f \right)  - \mu(f) \right] 
&= \sqrt{N}\left[ \frac{ \sum_{i=1}^N \frac{d\mu}{d\nu}(\xi^i)  f(\xi^i)}{ \sum_{j=1}^N \frac{d\mu}{d\nu}(\xi^j) }  - \mu(f) \right] \\
&= \frac{ \sqrt{N}   \left\{N^{-1} \sum_{i=1}^N \frac{d\mu}{d\nu}(\xi^i)  f(\xi^i) - \mu(f)\right\}  }{ N^{-1}\sum_{j=1}^N \frac{d\mu}{d\nu}(\xi^j) }   
\end{align*}

Numerator converges to $\text{Normal}\left[0, \sigma^2\left( \nu, f \right) \right]$ by CLT. Denominator converges to $1$ by SLLN. Ratio converges to $\text{Normal}\left[0, \sigma^2\left( \nu, f \right) \right]$ by Slutsky's. 


## Example: approximating a posterior mean

Assume 

- a likelihood: $y \sim \text{Normal}(\theta,1)$, and 

- a prior: $p(\theta) = \frac{1}{\pi(1+\theta^2)}$. 

Approximate the mean of the posterior using the prior distribution as a proposal. Keep in mind that we can only evaluate an unnormalized posterior:

$$
p(\theta \mid y) \propto p(y \mid \theta) p(\theta)
$$



## Example: approximating a posterior mean

We can only evaluate an unnormalized posterior:

$$
p(\theta \mid y) \propto p(y \mid \theta) p(\theta)
$$

If we sample from $p(\theta) = \frac{1}{\pi(1+\theta^2)}$ then the unnormalized weights are
\begin{align*}
\frac{\prod_{i=1}^n p(y_i \mid \theta_j) p(\theta_j)}{p(\theta_j)}
&= \prod_{i=1}^n p(y_i \mid \theta_j )  \\
&= 2\pi^{-n/2} \exp\left[-\frac{1}{2} \sum_i(y_i-\theta_j)^2 \right]
\end{align*}


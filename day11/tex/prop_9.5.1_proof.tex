\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{bbm} %for bold indicator func

\title{A More Detailed Proof of Proposition 9.5.1}

\begin{document}
\maketitle

Overall, we have
$$
\sum_i U_{N,i} = \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N]  + \left(\sum_i \bar{U}_{N,i} - \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N] \right) +  \left(\sum_i U_{N,i} - \sum_i \bar{U}_{N,i}\right).
$$
We want to show each of these pieces on the right hand side converges to $0$. If they all do, then we can use triangle inequality to show the left hand side converges to $0$.


\subsubsection*{Part 1}

Starting with the first summand on the right hand side, we have
\[
0 = E[U_{N,i} \mid \mathcal{F}^N] = E[U_{N,i}\mathbbm{1}(U_{N,i} \ge \epsilon) \mid \mathcal{F}^N] + E[\bar{U}_{N,i}\mid \mathcal{F}^N ]
\]
summing both sides gives us
\[
0 = \sum_{i=1}^{M_N} E[U_{N,i} \mathbbm{1}(U_{N,i} \ge \epsilon) \mid \mathcal{F}^N] + \sum_{i=1}^{M_N} E[\bar{U}_{N,i}\mid \mathcal{F}^N ].
\]

Subtracting the rightmost portion from both sides and then applying an absolute value to both sides gives us
\[
\left|\sum_{i=1}^{M_N} E[\bar{U}_{N,i}\mid \mathcal{F}^N ] \right| = \left| \sum_{i=1}^{M_N} E[U_{N,i}1(U_{N,i} \ge \epsilon) \mid \mathcal{F}^N] \right| \le \sum_{i=1}^{M_N} E[|U_{N,i}| \mathbbm{1}(U_{N,i} \ge \epsilon) \mid \mathcal{F}^N] .
\]
The last inequality arises after we use the triangle inequality, and Jensen's inequality. 

Then, taking the limit on both sides and using assumption 9.6.3,
\[
\sum_{i=1}^{M_N} E[\bar{U}_{N,i}\mid \mathcal{F}^N ] \overset{\text{p}}{\to} 0 \tag{1} 
\]
as $N \to \infty$.

\subsubsection*{Part 2}

Expanding a little on the Chebyshev's thing at the top of page 334:
\begin{align*}
0 &\le A_N(\delta) \\
&= P\left(\left| \sum_{i=1}^{M_N} \bar{U}_{N,i} - \sum_{i=1}^{M_N} E\left[\bar{U}_{N,i} \mid \mathcal{F}^N \right] \right| \ge \delta \bigg| \mathcal{F}^N \right) \tag{defn.} \\
&\le \delta^{-2} \text{Var}\left( \sum_i^{M_N} \bar{U}_{N,i} \mid \mathcal{F}^N \right) \tag{Chebyshev's} \\
&= \delta^{-2} \sum_i \sum_j \text{Cov}\left(\bar{U}_{N,i},\bar{U}_{N,j}  \mid \mathcal{F}^N  \right) \\
&= \delta^{-2}  \sum_i \text{Var}\left(\bar{U}_{N,i}  \mid \mathcal{F}^N  \right) \tag{independence} \\
&= \delta^{-2}  \sum_i \left\{  E\left(\bar{U}_{N,i}^2  \mid \mathcal{F}^N  \right) - [E\left(\bar{U}_{N,i}  \mid \mathcal{F}^N  \right) ]^2 \right\} \\
&\le \delta^{-2}  \sum_i E\left(\bar{U}_{N,i}^2  \mid \mathcal{F}^N  \right)  \\
&\overset{p}{\to} 0 \tag{9.62}.
\end{align*}
So this is where 9.6.2 applies, giving us $A_N(\delta) \to 0$ in probability.

Because $A_N(\delta) \le 1$ (it's a probability), we have by dominated convergence
\[
 \lim_N E[A_N(\delta)] = E[\lim_N A_N(\delta)] = 0.
\]
This is equivalent to (9.65) on page 334, and so 
$$
\sum_i \bar{U}_{N,i} - \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N]  \overset{\text{p}}{\to} 0.
$$

If you're thinking that we don't need dominated convergence, notice that this last statement is about unconditional expectations, not conditional expectations, which is what $ A_N(\delta)$ is.

\subsubsection*{Part 3}


\begin{align*}
P\left(\left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i} \right| \ge \delta \bigg\rvert \mathcal{F}^N \right) 
&= P\left(\left| \sum_i U_{N,i} \mathbbm{1}(U_{N,i} \ge \epsilon ) \right| \ge \delta \bigg\rvert \mathcal{F}^N \right) \\
&\le P\left( \sum_i\left| U_{N,i}\right| \mathbbm{1}(U_{N,i} \ge \epsilon )   \ge \delta \bigg\rvert \mathcal{F}^N \right) \tag{tri-ineq} \\
&\le \delta^{-1} \sum_i E\left[\left| U_{N,i}\right| \mathbbm{1}(U_{N,i} \ge \epsilon )  \bigg\rvert \mathcal{F}^N \right] \tag{Markov's and linearity} \\
&\to 0 \tag{9.63}.
\end{align*}

So the *conditional* probailities go to $0$. To show that the unconditional ones do, as well, we need to iterate the expectations, and use the same argument of dominated convergence as before:
\begin{align*}
\lim_N P\left(\left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i} \right| \ge \delta  \right) 
&= \lim_N E\left[ \mathbbm{1}\left(\left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i} \right| \ge \delta  \right) \right] \\
&= \lim_N E\left[ E\left\{ \mathbbm{1} \left( \left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i} \right| \ge \delta \right)  \mid \mathcal{F}^N \right\}    \right] \tag{law total expec.} \\
&=  E\left[  \lim_N E \mathbbm{1}\left( \left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i} \right| \ge \delta  \mid \mathcal{F}^N  \right)  \right] \tag{DCT}\\
&= 0 \tag{previous}.
\end{align*}

\subsubsection*{Tying it all together}

Recall that
\begin{align*}
\sum_i U_{N,i} &= \left(\sum_i U_{N,i} - \sum_i \bar{U}_{N,i}\right) + \left(\sum_i \bar{U}_{N,i} - \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N] \right) +  \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N].
\end{align*}

Using the triangle inequality we have that

\begin{align*}
\bigg\rvert \sum_i U_{N,i} \bigg\rvert  &\le \left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i}\right| + \left|\sum_i \bar{U}_{N,i} - \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N] \right| +  \bigg\rvert \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N]\bigg\rvert .
\end{align*}

so

\begin{align*}
\left\{ \bigg\rvert \sum_i U_{N,i} \bigg\rvert  \ge \delta \right\}  &\subseteq \left\{ \left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i}\right| \ge \delta/3 \right\} \\
&\cup \left\{ \left|\sum_i \bar{U}_{N,i} - \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N] \right| \ge \delta/3 \right\} \\
&\cup \left\{ \bigg\rvert \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N]\bigg\rvert \ge \delta/3 \right\}.
\end{align*}

so

\begin{align*}
P\left( \bigg\rvert \sum_i U_{N,i} \bigg\rvert  \ge \delta \right)  &\le P\left( \left| \sum_i U_{N,i} - \sum_i \bar{U}_{N,i}\right| \ge \delta/3 \right) \\
& + P \left( \left|\sum_i \bar{U}_{N,i} - \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N] \right| \ge \delta/3 \right) \\
&+ P \left( \bigg\rvert \sum_i E[ \bar{U}_{N,i}  \mid \mathcal{F}^N]\bigg\rvert \ge \delta/3 \right ).
\end{align*}

by sub-additivity of $P$, and because probability of supersets is greater than subsets.

Use parts (1), (2) and (3), the three terms on the right hand side all go to $0$ in probability. 


\end{document}
---
title: "Day 21"
author: "Taylor R. Brown"
date: "1/6/2020"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Roadmap

9.2

- sampling importance resampling/IS with resampling/factored sampling 

9.3

- **sequential importance sampling with resampling (single-step analysis)**

9.4

- sequential importance sampling with resampling (multi-step analysis)

## Overview

We will investigate SISR in detail, but we will disregard "iid sampling" which is rarely available in practice.

In earlier chapters, we separated everything along the lines of re-weighting and resampling. Now we'll address **mutation,** which is the same as sampling, but the samples are drawn from a Markov kernel. 

## Motivation

Want 
$$
\mu(f) = \frac{\int \nu(dx) L(x, dy) f(y)}{\int \nu(dx) L(x, dy) }
$$

- sample from $\nu$

- evaluate the density of $L$

So we use

$$
\frac{\int \nu(dx) L(x, dy) f(y)}{\int \nu(dx) L(x, dy) } = \frac{\int \nu(dx) \left[\frac{L(x,\cdot)}{R(x, \cdot)}(y)\right]R(x, dy) f(y)}{\int \nu(dx) \left[\frac{L(x,\cdot)}{R(x, \cdot)}(y)\right] R(x, dy) }
$$

## Some Notation

- $L(x,A)$ is "like" $\int_A g(y_t \mid x_t)f(x_t \mid x_{t-1} = x) d x_t$

- $R(x,A)$ is "like" $\int_A q_t(x_t \mid y_t, x) dx_t$

- $\frac{L(x,A)}{L(x,\mathsf{X})}$ is "like" $\frac{\int_A g(y_t \mid x_t)f(x_t \mid x) d x_t}{\int_{\mathsf{X}} g(y_t \mid x_t)f(x_t \mid x) d x_t} = p(x_t \in A \mid y_t, x_{t-1} = x)$ (optimal proposal)

- $\frac{dL(x_{t-1}, \cdot)}{dR(x_{t-1}, \cdot)}(x_t)$ is "like" $\frac{g(y_t \mid x_t)f(x_t \mid x_{t-1}) }{q_t(x_t \mid y_t, x_{t-1})}$


## Some Notation


- $\nu$ samples from the previous time period that were used to target the old target

- $\mu = (\nu L)(A)/(\nu L)(\mathsf{X})$ target at next time point


- $\tilde{\xi}^{N,i}$ are no longer resampled particles, but rather the "mutated" particles 

- $\tilde{\xi}^{N,i} \mid \xi^{N,i} \sim  R$ (mutated from the old samples using $R$)

- $\tilde{\xi}^{N,i} \sim \nu R$  


## Assumptions

Assumption 9.3.1: $0 < \int \nu(dx) L(x,\mathsf{X}) < \infty$.

Assumption 9.3.2: $\{(\xi^{N,i},1)\}_{1 \le i \le M_N}$ are consistent for $(\nu, \mathsf{C})$. Also $L(x,\mathsf{X}) \in \mathsf{C}$.

Assumption 9.3.3: $\forall x \in \mathsf{X}$, $L(x, \cdot) \ll R(x, \cdot)$, and there exists a strictly positive RN derivative: $\frac{dL(x,\cdot )}{dR(x, \cdot)}$.


## Mutation!

Let $\{\alpha_N\} \in \mathbb{N}$ be the number of "offspring" of each "parent" (not the same as $a_N$ from the previous section). We need this because we set $\tilde{M}_N = \alpha_N M_N$.

- $i=1,\ldots, M_N$, and for each $i$, we have 
$$
j = \alpha_N(i-1) + 1, \ldots, \alpha_Ni
$$

If $\alpha_N = 1$, we don't need multiple indexes, which is usually the way I implement everything in practice.

## Mutation! (continued)


1. Draw $\tilde{\xi}^{N,1}, \ldots, \tilde{\xi}^{N,\tilde{M}_N}$ conditionally independently given $\mathcal{F}^N = \mathcal{G}^N \bigvee \sigma\left(\xi^{N,i}, \ldots, \xi^{N,M_N} \right)$ as follows:
$$
\tilde{\xi}^{N,j} \sim R(\xi^{N,i}, \cdot)
$$

2. Assign each sample $\tilde{\xi}^{N,j}$ the weight
$$
\tilde{\omega}^{N,j} = \frac{dL(\xi^{N,i},\cdot )}{dR(\xi^{N,i}, \cdot)}(\tilde{\xi}^{N,j})
$$


## Theorem 9.3.5

Assume 

1. Assumption 9.3.1: $0 < L(x,\mathsf{X}) < \infty$.

2. Assumption 9.3.2: $\{(\xi^{N,i},1)\}_{1 \le i \le M_N}$ are consistent for $(\nu, \mathsf{C})$. $L(x,\mathsf{X}) \in \mathsf{C}$.

3. Assumption 9.3.3: $\forall x \in \mathsf{X}$, $L(x, \cdot) \ll R(x, \cdot)$, and there exists a strictly positive RN derivative: $\frac{dL(x,\cdot )}{dR(x, \cdot)}$.

and define 
$$
\tilde{\mathsf{C}} = \left\{ f \in L^1(\mathsf{X}, \mu) : x \mapsto L(x, |f|) \in \mathsf{C}  \right\}.
$$

Then $\tilde{\mathsf{C}}$ is proper and $\{ (\tilde{\xi}^{N,i}, \tilde{\omega}^{N,i} )\}_{1 \le i \le M_N}$ is consistent for $(\mu, \tilde{\mathsf{C}})$.



## Theorem 9.3.5: proof

Check $\tilde{\mathsf{C}}$ is proper on your own.

The rest of the proof is just applying theorem 9.5.7, and using a trick:

\begin{align*}
L(\xi^{N,i}, f) &= 
\int L(\xi^{N,i}, dy) f(y) \\
&= \int  R(\xi^{N,i},dy) \frac{d L(\xi^{N,i}, \cdot)}{ dR(\xi^{N,i}, \cdot) }(y) f(y) \\
&= E\left[ \tilde{\omega}^{N,i} f(\tilde{\xi}^{N,j}) \mid \mathcal{F}^N \right]
\end{align*}


## Theorem 9.3.5: proof (1 of )

Recall 

$$
\mu(f)  = \frac{\int \nu(dx) L(x, dy) f(y)}{\int \nu(dx) L(x, dy) } = \frac{\int \nu(dx) \left[\frac{L(x,\cdot)}{R(x, \cdot)}(y)\right]R(x, dy) f(y)}{\int \nu(dx) \left[\frac{L(x,\cdot)}{R(x, \cdot)}(y)\right] R(x, dy) } 
$$
We want to show 

1. $\frac{1}{\tilde{M}^N} \sum_{j=1}^{\tilde{M}_N}  \tilde{\omega}^{N,j} 1(\tilde{\xi}^{N,j})\overset{\text{p}}{\to} \nu L(\mathsf{X})$

2. $\frac{1}{\tilde{M}^N} \sum_{j=1}^{\tilde{M}_N} \tilde{\omega}^{N,j} f(\tilde{\xi}^{N,j})\overset{\text{p}}{\to} \nu L(f)$

and thus their ratio converges to $\mu(f)$.


## Theorem 9.3.5: proof ( of )

Actually, (1) is a special case of (2). This is because, Assumption 9.3.2 gives us $L(x,\mathsf{X}) \in \mathsf{C}$, which means the identity function $1 \in \tilde{\mathsf{C}}$. 

So we pick an arbitrary $f \in \tilde{\mathsf{C}}$ and show 


$$
\frac{1}{\tilde{M}^N} \sum_{j=1}^{\tilde{M}_N} \tilde{\omega}^{N,j} f(\tilde{\xi}^{N,j})\overset{\text{p}}{\to} \nu L(f)
$$

For the details of this, see `proof_9.3.5.pdf`


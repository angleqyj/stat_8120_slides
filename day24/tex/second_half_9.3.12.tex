\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{bbm} %for bold indicator func

\title{A More Detailed Proof of the Second Half of Theorem 9.3.12}

\begin{document}
\maketitle

\section{What we have}

Our assumptions are

\begin{enumerate}

\item Assumption 9.3.1: $0 < L(x,\mathsf{X}) < \infty$.

\item Assumption 9.3.2: $\{(\xi^{N,i},1)\}_{1 \le i \le M_N}$ are consistent for $(\nu, \mathsf{C})$. $L(x,\mathsf{X}) \in \mathsf{C}$.

\item Assumption 9.3.3: $\forall x \in \mathsf{X}$, $L(x, \cdot) \ll R(x, \cdot)$, and there exists a strictly positive RN derivative: $\frac{dL(x,\cdot )}{dR(x, \cdot)}$.

\item Assumption 9.3.6 (new) $\{ (\xi^{N,i} , 1)\}_{1 \le i \le M_N}$ is asymptotically normal for $(\nu, \mathsf{A}, \sigma, \{M_N^{1/2}\})$, where $\mathsf{A}$ is a proper set, and $\sigma$ is a nonnegative function on $\mathsf{A}$.

\item $\{\alpha_N\}$ has a possibly-infinite limit $\alpha$.

\end{enumerate}

We also have the things we have shown already in the first half of the proof:

\begin{enumerate}

\item 
$$
\tilde{\mathsf{A}} = 
\left\{ 
f \in L^2(\mathsf{X}, \mu) : 
x \mapsto L(x,|f|) \in \mathsf{A},
x \mapsto \int R(x,dx') \left[ \frac{dL(x,\cdot)}{dR(x,\cdot)}(x') f(x')\right]^2 \in \mathsf{C}
\right\}
$$ is proper

\item $\{(\tilde{\xi}^{N,j}, \tilde{\omega}^{N,i} ) \}_{1 \le j \le \tilde{M}_N}$ is asymptotically normal for $(\mu, \tilde{\mathsf{A}}, \tilde{\sigma}, \{M_N^{1/2}\})$ with
$$
\tilde{\sigma}(f)  =
\frac{\sigma^2\left( L[f-\mu(f)] \right) + \alpha^{-1} \eta^2\left(f - \mu(f) \right)}{[\nu L(\mathsf{X})]^2 }
$$
\end{enumerate}

\section{Starting Off}

We need to use the above to show that 
$\{ ( \check{\xi}^{N,i}, 1 )\}_{1 \le i \le \tilde{M}_N}$ is asymptotically normal for $(\mu, \tilde{\mathsf{A}}, \check{\sigma}, \{M_N^{1/2}\})$ with 
$$
\check{\sigma}(f) = 
\text{Var}_{\mu}(f) + \tilde{\sigma}(f) .
$$



In other words, if we pick $f\in \tilde{\mathsf{A}}$, we want to show that 
$$
\sqrt{M_N} \left[ M_N^{-1}\sum_{i=1}^{M_N} \left\{  f(\check{\xi}^{N,i}) - \mu(f) \right\}    \right]
$$

converges in distribution to a mean-zero normal random variable. 

The main trick is splitting it up into two pieces by adding and subtracting conditional expectations, which condition on all the information that is had just before resampling is conducted:

\begin{align*}
&\sqrt{M_N} \left[ M_N^{-1}\sum_{i=1}^{M_N} \left\{  f(\check{\xi}^{N,i}) - \mu(f) \right\}    \right] \\
&= \sqrt{M_N} \left[ M_N^{-1} \sum_{i=1}^{M_N} \left\{  f(\check{\xi}^{N,i}) - E[f(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] \right\} + M_N^{-1}\sum_{i=1}^{M_N} \left\{  E[f(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] - \mu(f) \right\}    \right] \\
&= \sqrt{M_N} \left[ B_N + A_N    \right]
\end{align*}

The second piece will converge by a consequence that we have already proved. The first will converge by the more interesting line of reasoning.

\section{Handling $A_N$}

Let's work on the second piece first, just to get it out of the way:

\begin{align*}
\sqrt{\tilde{M}_N}A_N &= 
\sqrt{\tilde{M}_N} M_N^{-1}\sum_{i=1}^{M_N} \left\{  E[f(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] - \mu(f) \right\}   \\
&=   \sqrt{\tilde{M}_N}\left\{ E[f(\check{\xi}^{N,3}) \mid \check{\mathcal{F}}^N] - \mu(f)\right\} \tag{identicalness}\\
&= \sqrt{\tilde{M}_N}\left[ \sum_{j=1}^{\tilde{M}_N} \frac{ \frac{dL(\xi^{N,i}, \cdot)}{dR(\xi^{N,i}, \cdot)}(\tilde{\xi}^{N,j})  }{ \sum_{j'} \frac{dL(\xi^{N,i'}, \cdot)}{dR(\xi^{N,i'}, \cdot)}(\tilde{\xi}^{N,j'}) } f( \tilde{\xi}^{N,j})  - \mu(f) \right]
\end{align*}

Fortunately we have already shown the second consequence in this theorem, so we can use that: $\{(\tilde{\xi}^{N,j}, \tilde{\omega}^{N,i} ) \}_{1 \le j \le \tilde{M}_N}$ is asymptotically normal for $(\mu, \tilde{\mathsf{A}}, \tilde{\sigma}, \{M_N^{1/2}\})$. So we can automatically say this:

$$
\sqrt{\tilde{M}_N} A_N \overset{\text{D}}{\to} \text{Normal}(0, \tilde{\sigma}^2(f)).
$$

\section{Bringing in $B_N$}

Now we will show 
$$
\sqrt{M_N}  M_N^{-1} \sum_{i=1}^{M_N} \left\{  f(\check{\xi}^{N,i}) - E[f(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] \right\}
$$

converges in distribution to a mean-zero normal distribution. We will use 9.5.13 after having made the following notational conversions:

\begin{enumerate}
\item $\xi^{N,i}$ into $\check{\xi}^{N,i}$
\item $\mathcal{F}^N$ into $\check{\mathcal{F}}^N$
\end{enumerate} 


\subsection{verifying 9.5.13.i}

Clearly the triangular array is conditionally independent. This is guaranteed by the description of resampling. Also, 
$$
E[f^2(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] 
= \sum_{j=1}^{\tilde{M}_N} \frac{\frac{dL(\xi^{N,i},\cdot)}{dR(\xi^{N,i},\cdot)}(\tilde{\xi}^{N,j}) }{ \sum_{j'}\frac{dL(\xi^{N,i'},\cdot)}{dR(\xi^{N,i'},\cdot)}(\tilde{\xi}^{N,j'}) }f^2(\tilde{\xi}^{N,i}) 
$$

$\sum_{j'}\frac{dL(\xi^{N,i'},\cdot)}{dR(\xi^{N,i'},\cdot)}(\tilde{\xi}^{N,j'}) > 0$ by assumption 9.3.3. $f < \infty$ almost surely because $f \in \tilde{\mathsf{A}} \subset L^2(\mathsf{X}, \mu)$, so $f^2$ is finite. Last, $L(x,\mathsf{X}) < \infty$ by assumption 9.3.1, which means 
$$
\infty > \nu L(\mathsf{X}) = \iint \nu(dx) R(x,dx') \frac{dL(x,)}{dR(x,)}(x')
$$
which means $dL/dR$ is finite almost everywhere, too.

\subsection{verifying 9.5.13.ii}

The average conditional variance must converge to a positive constant. The average conditional variance is the same as the first, by identicalness, and that splits up into two pieces:
$$
E[f^2(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] - \left( E[f(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] \right)^2
$$

Assumptions 9.3.1, 9.3.2 and 9.3.3 guarantee Theorem 9.3.5, which implies
\begin{align*}
E[f^2(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] 
&= \sum_{j=1}^{\tilde{M}_N} \frac{\frac{dL(\xi^{N,i},\cdot)}{dR(\xi^{N,i},\cdot)}(\tilde{\xi}^{N,j}) }{ \sum_{j'}\frac{dL(\xi^{N,i'},\cdot)}{dR(\xi^{N,i'},\cdot)}(\tilde{\xi}^{N,j'}) }f^2(\tilde{\xi}^{N,i}) \\
&\overset{\text{p}}{\to} \nu L(f^2) / \nu L(\mathsf{X}) = \mu(f^2)
\end{align*}

and

\begin{align*}
E[f(\check{\xi}^{N,i}) \mid \check{\mathcal{F}}^N] 
&= \sum_{j=1}^{\tilde{M}_N} \frac{\frac{dL(\xi^{N,i},\cdot)}{dR(\xi^{N,i},\cdot)}(\tilde{\xi}^{N,j}) }{ \sum_{j'}\frac{dL(\xi^{N,i'},\cdot)}{dR(\xi^{N,i'},\cdot)}(\tilde{\xi}^{N,j'}) }f(\tilde{\xi}^{N,i}) \\
&\overset{\text{p}}{\to}  \mu(f)
\end{align*}

And so 
$$
\frac{1}{M_N} \left( 
\sum_{i=1}^{M_N} \text{Var}\left[ 
f(\check{\xi}^{N,1})
\mid \check{\mathcal{F}}^N
\right]
\right)
\overset{\text{p}}{\to}
\sigma^2 > 0.
$$

\subsection{verifying 9.5.13.iii}

Use a similar dominated convergence argument to show that this is true. 

All three of these conditions, because they now have been verified, guarantee that

$$
\sqrt{M_N}  B_N \overset{\text{D}}{\to} \text{Normal}(0, \text{Var}_{\mu}(f)).
$$


\section{The asymptotic joint distribution}

The final consequence we want is implied by the following:
$$
\begin{bmatrix}
\sqrt{M_N} B_N \\
\sqrt{\tilde{M}_N} A_N
\end{bmatrix}
$$

converges in distribution to a multivariate normal distribution. We will show this by showing that the joint characteristic function converges to the multivariate normal's characteristic function.


Pick $u,v \in \mathbb{R}$, and look at the bivariate characteristic function. The trick is to iterate expectations:

\begin{align*}
E\left[\exp\left(i \left\{ u \sqrt{M_N} B_N + v \sqrt{\tilde{M}_N} A_N \right\} \right) \right]
&= E\left[ E\left[ \exp\left(i \left\{ u \sqrt{M_N} B_N + v \sqrt{\tilde{M}_N} A_N \right\} \right) \bigg\rvert \mathcal{F}^N\right] \right] \\
&= E\left[ E\left[ \exp\left(i u\sqrt{M_N} B_N \right) \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) \bigg\rvert \mathcal{F}^N\right] \right] \\
&= E\left[ \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) E\left[ \exp\left(i u \sqrt{M_N} B_N \right)  \bigg\rvert \mathcal{F}^N\right] \right] .
\end{align*}

Then we take the limit as $N \to \infty$

\begin{align*}
&\lim_{N \to \infty} E\left[\exp\left(i \left\{ u \sqrt{M_N} B_N + v \sqrt{\tilde{M}_N} A_N \right\} \right) \right] \\
&= \lim_{N \to \infty} E\left[ \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) E\left[ \exp\left(i u \sqrt{M_N} B_N \right)  \bigg\rvert \mathcal{F}^N\right] \right] \tag{previous} \\
&=  E\left[ \lim_{N \to \infty} \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) E\left[ \exp\left(i u \sqrt{M_N} B_N \right)  \bigg\rvert \mathcal{F}^N\right] \right] \tag{DCT} \\
&=  E\left[ \lim_{N \to \infty} \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) \lim_{N \to \infty}E\left[ \exp\left(i u \sqrt{M_N} B_N \right)  \bigg\rvert \mathcal{F}^N\right] \right]  \tag{limit properties} \\
&= E\left[ \lim_{N \to \infty} \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) \exp\left[-\frac{u^2 \text{Var}_{\mu}(f) }{2} \right] \right] \tag{Theorem 9.5.13} \\
&= \exp\left[-\frac{u^2 \text{Var}_{\mu}(f) }{2} \right]  E\left[ \lim_{N \to \infty} \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) \right] \tag{linearity of $E$} \\
&= \exp\left[-\frac{u^2 \text{Var}_{\mu}(f) }{2} \right] \lim_{N \to \infty}   E\left[ \exp\left(i v \sqrt{\tilde{M}_N} A_N  \right) \right] \tag{DCT} \\
&= \exp\left[-\frac{u^2 \text{Var}_{\mu}(f) }{2} \right] \exp\left[ - \frac{u^2  \tilde{\sigma}^2(f) }{2} \right] \tag{Previous subsection}. \\
\end{align*}

\section{Final Step}

Take the following inner product and apply the multivariate delta method:

\begin{align*}
\sqrt{M_N} \left[ B_N + A_N    \right]
&= 
\begin{bmatrix}
1 & \sqrt{M}_N/\sqrt{\tilde{M}_N}
\end{bmatrix}
\begin{bmatrix}
\sqrt{M_N} B_N \\
\sqrt{\tilde{M}_N} A_N
\end{bmatrix} \\
& \overset{\text{D}}{\to}
\text{Normal}\left( 
0,
\text{Var}_{\mu}(f) + \frac{1}{\alpha} \tilde{\sigma}^2\left(f \right)
\right) 
\end{align*}



\end{document}